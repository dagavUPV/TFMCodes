{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y h5py"
      ],
      "metadata": {
        "id": "T_YHQja-GIbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'h5py < 3.0.0'"
      ],
      "metadata": {
        "id": "V2W9iQ_IGK5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://stackoverflow.com/questions/73215696/did-colab-suspend-tensorflow-1-x"
      ],
      "metadata": {
        "id": "91gFTHb81tHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow"
      ],
      "metadata": {
        "id": "l5Jxup7O05Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.15.2"
      ],
      "metadata": {
        "id": "IncHSQtyEblD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "# python standard libraries\n",
        "import os\n",
        "import random\n",
        "import fnmatch\n",
        "import datetime\n",
        "import pickle\n",
        "\n",
        "# data processing\n",
        "import numpy as np\n",
        "np.set_printoptions(formatter={'float_kind':lambda x: \"%.4f\" % x})\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.width', 300)\n",
        "pd.set_option('display.float_format', '{:,.4f}'.format)\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "# tensorflow\n",
        "#indicar la versiÃ³n a utilizar\n",
        "#%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "from tensorflow  import keras\n",
        "import h5py\n",
        "from tensorflow import keras as k2\n",
        "from tensorflow.keras.models import Sequential  # V2 is tensorflow.keras.xxxx, V1 is keras.xxx\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "print( f'tf.__version__: {tf.__version__}' )\n",
        "print( f'tf_keras.__version__: {k2.__version__}' )\n",
        "print( f'keras.__version__: {keras.__version__}' )\n",
        "print( f'h5py.__version__: {h5py.__version__}' )\n",
        "\n",
        "# sklearn\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# imaging\n",
        "import cv2\n",
        "from imgaug import augmenters as img_aug\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "VUlWVGhTg0qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf87RIinawB0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "model_output_dir = '/content/gdrive/My Drive/Colab Notebooks/roadCamCar/output'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import images\n",
        "!cd /content\n",
        "#!git clone https://github.com/dctian/DeepPiCar\n",
        "\n",
        "!ls\n",
        "data_dir = '/content/gdrive/My Drive/Colab Notebooks/data/roadCamCar'\n",
        "file_list = os.listdir(data_dir)\n",
        "image_paths = []\n",
        "steering_angles = []\n",
        "pattern = \"*.jpg\"\n",
        "for filename in file_list:\n",
        "    if fnmatch.fnmatch(filename, pattern):\n",
        "        image_paths.append(os.path.join(data_dir,filename))\n",
        "        print(filename.split(\"_\")[-1].split(\".\")[0])\n",
        "        angle = int(filename[-7:-4])  # 092 part of video01_143_092.png is the angle. 90 is go straight\n",
        "        angle = int(filename.split(\"_\")[-1].split(\".\")[0])\n",
        "        steering_angles.append(angle)\n",
        "\n",
        "image_index = 20\n",
        "plt.imshow(Image.open(image_paths[image_index]))\n",
        "print(\"image_path: %s\" % image_paths[image_index] )\n",
        "print(\"steering_Angle: %d\" % steering_angles[image_index] )\n",
        "df = pd.DataFrame()\n",
        "df['ImagePath'] = image_paths\n",
        "df['Angle'] = steering_angles"
      ],
      "metadata": {
        "id": "3KDxy_mzhQG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the distribution of steering angle\n",
        "num_of_bins = 25\n",
        "samples_per_bin = 400\n",
        "hist, bins = np.histogram(df['Angle'], num_of_bins)\n",
        "\n",
        "fig, axes = plt.subplots(1,1, figsize=(12,4))\n",
        "axes.hist(df['Angle'], bins=num_of_bins, width=1, color='blue')"
      ],
      "metadata": {
        "id": "Nsdg9gHMhUmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split( image_paths, steering_angles, test_size=0.2)\n",
        "print(\"Training data: %d\\nValidation data: %d\" % (len(X_train), len(X_valid)))\n",
        "\n",
        "# plot the distributions of train and valid, make sure they are consistent\n",
        "fig, axes = plt.subplots(1,2, figsize=(12,4))\n",
        "axes[0].hist(y_train, bins=num_of_bins, width=1, color='blue')\n",
        "axes[0].set_title('Training Data')\n",
        "axes[1].hist(y_valid, bins=num_of_bins, width=1, color='red')\n",
        "axes[1].set_title('Validation Data')"
      ],
      "metadata": {
        "id": "0hRmSj0VhWrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_imread(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    return image\n",
        "\n",
        "def zoom(image):\n",
        "    zoom = img_aug.Affine(scale=(1, 1.3))  # zoom from 100% (no zoom) to 130%\n",
        "    image = zoom.augment_image(image)\n",
        "    return image\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
        "image_orig = my_imread(image_paths[image_index])\n",
        "image_zoom = zoom(image_orig)\n",
        "axes[0].imshow(image_orig)\n",
        "axes[0].set_title(\"orig\")\n",
        "axes[1].imshow(image_zoom)\n",
        "axes[1].set_title(\"zoomed\")"
      ],
      "metadata": {
        "id": "PdKNWF0QhYg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pan(image):\n",
        "    # pan left / right / up / down about 10%\n",
        "    pan = img_aug.Affine(translate_percent= {\"x\" : (-0.1, 0.1), \"y\": (-0.1, 0.1)})\n",
        "    image = pan.augment_image(image)\n",
        "    return image\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
        "image_orig = my_imread(image_paths[image_index])\n",
        "image_pan = pan(image_orig)\n",
        "axes[0].imshow(image_orig)\n",
        "axes[0].set_title(\"orig\")\n",
        "axes[1].imshow(image_pan)\n",
        "axes[1].set_title(\"panned\")"
      ],
      "metadata": {
        "id": "40cKYzZ1haO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_brightness(image):\n",
        "    # increase or decrease brightness by 30%\n",
        "    brightness = img_aug.Multiply((0.7, 1.3))\n",
        "    image = brightness.augment_image(image)\n",
        "    return image\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
        "image_orig = my_imread(image_paths[image_index])\n",
        "image_brightness = adjust_brightness(image_orig)\n",
        "axes[0].imshow(image_orig)\n",
        "axes[0].set_title(\"orig\")\n",
        "axes[1].imshow(image_brightness)\n",
        "axes[1].set_title(\"brightness adjusted\")"
      ],
      "metadata": {
        "id": "Nt86NhPChb61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def blur(image):\n",
        "    kernel_size = random.randint(1, 5)  # kernel larger than 5 would make the image way too blurry\n",
        "    image = cv2.blur(image,(kernel_size, kernel_size))\n",
        "   \n",
        "    return image\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
        "image_orig = my_imread(image_paths[image_index])\n",
        "image_blur = blur(image_orig)\n",
        "axes[0].imshow(image_orig)\n",
        "axes[0].set_title(\"orig\")\n",
        "axes[1].imshow(image_blur)\n",
        "axes[1].set_title(\"blurred\")"
      ],
      "metadata": {
        "id": "-VR3u9eqhdtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_flip(image, steering_angle):\n",
        "    is_flip = random.randint(0, 1)\n",
        "    if is_flip == 1:\n",
        "        # randomly flip horizon\n",
        "        image = cv2.flip(image,1)\n",
        "        steering_angle = 180 - steering_angle\n",
        "   \n",
        "    return image, steering_angle\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
        "image_orig = my_imread(image_paths[image_index])\n",
        "image_flip, steering_angle = random_flip(image_orig, steering_angles[image_index])\n",
        "axes[0].imshow(image_orig)\n",
        "axes[0].set_title(\"orig\")\n",
        "axes[1].imshow(image_flip)\n",
        "axes[1].set_title(\"flipped, angle=%s\" % steering_angle)"
      ],
      "metadata": {
        "id": "daBvsLpLhfbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# put it together\n",
        "def random_augment(image, steering_angle):\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = pan(image)\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = zoom(image)\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = blur(image)\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = adjust_brightness(image)\n",
        "    image, steering_angle = random_flip(image, steering_angle)\n",
        "    \n",
        "    return image, steering_angle\n",
        "\n",
        "# show a few randomly augmented images\n",
        "ncol = 2\n",
        "nrow = 10\n",
        "fig, axes = plt.subplots(nrow, ncol, figsize=(15, 50))\n",
        "\n",
        "for i in range(nrow):\n",
        "    rand_index = random.randint(0, len(image_paths) - 1)\n",
        "    image_path = image_paths[rand_index]\n",
        "    steering_angle_orig = steering_angles[rand_index]\n",
        "    \n",
        "    image_orig = my_imread(image_path)\n",
        "    image_aug, steering_angle_aug = random_augment(image_orig, steering_angle_orig)\n",
        "    \n",
        "    axes[i][0].imshow(image_orig)\n",
        "    axes[i][0].set_title(\"original, angle=%s\" % steering_angle_orig)\n",
        "    axes[i][1].imshow(image_aug)\n",
        "    axes[i][1].set_title(\"augmented, angle=%s\" % steering_angle_aug)"
      ],
      "metadata": {
        "id": "oX5z4qVWhhCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def img_preprocess(image):\n",
        "    height, _, _ = image.shape\n",
        "    #image = image[int(height/2):,:,:]  # remove top half of the image, as it is not relavant for lane following\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)  # Nvidia model said it is best to use YUV color space\n",
        "    image = cv2.GaussianBlur(image, (3,3), 0)\n",
        "    image = cv2.resize(image, (200,66)) # input image size (200,66) Nvidia model\n",
        "    image = image / 255 # normalizing, the processed image becomes black for some reason.  do we need this?\n",
        "    return image\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
        "image_orig = my_imread(image_paths[image_index])\n",
        "image_processed = img_preprocess(image_orig)\n",
        "axes[0].imshow(image_orig)\n",
        "axes[0].set_title(\"orig\")\n",
        "axes[1].imshow(image_processed)\n",
        "axes[1].set_title(\"processed\")"
      ],
      "metadata": {
        "id": "7FwJEhgRhitN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nvidia_model():\n",
        "    model = Sequential(name='Nvidia_Model')\n",
        "    \n",
        "    # elu=Expenential Linear Unit, similar to leaky Relu\n",
        "    # skipping 1st hiddel layer (nomralization layer), as we have normalized the data\n",
        "    \n",
        "    # Convolution Layers\n",
        "    model.add(Conv2D(24, (5, 5), strides=(2, 2), input_shape=(66, 200, 3), activation='elu')) \n",
        "    model.add(Conv2D(36, (5, 5), strides=(2, 2), activation='elu')) \n",
        "    model.add(Conv2D(48, (5, 5), strides=(2, 2), activation='elu')) \n",
        "    model.add(Conv2D(64, (3, 3), activation='elu')) \n",
        "    model.add(Dropout(0.2)) # not in original model. added for more robustness\n",
        "    model.add(Conv2D(64, (3, 3), activation='elu')) \n",
        "    \n",
        "    # Fully Connected Layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.2)) # not in original model. added for more robustness\n",
        "    model.add(Dense(100, activation='elu'))\n",
        "    model.add(Dense(50, activation='elu'))\n",
        "    model.add(Dense(10, activation='elu'))\n",
        "    \n",
        "    # output layer: turn angle (from 45-135, 90 is straight, <90 turn left, >90 turn right)\n",
        "    model.add(Dense(1)) \n",
        "    \n",
        "    # since this is a regression problem not classification problem,\n",
        "    # we use MSE (Mean Squared Error) as loss function\n",
        "    optimizer = Adam(lr=1e-3) # lr is learning rate\n",
        "    model.compile(loss='mse', optimizer=optimizer)\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "0uDJ-uWshkTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nvidia_model()\n",
        "print(model.summary())\n",
        "# check at we will have 252,219 trainable parameters"
      ],
      "metadata": {
        "id": "f6oOBzoN3dRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_data_generator(image_paths, steering_angles, batch_size, is_training):\n",
        "    while True:\n",
        "        batch_images = []\n",
        "        batch_steering_angles = []\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            random_index = random.randint(0, len(image_paths) - 1)\n",
        "            image_path = image_paths[random_index]\n",
        "            image = my_imread(image_paths[random_index])\n",
        "            steering_angle = steering_angles[random_index]\n",
        "            if is_training:\n",
        "                # training: augment image\n",
        "                image, steering_angle = random_augment(image, steering_angle)\n",
        "              \n",
        "            image = img_preprocess(image)\n",
        "            batch_images.append(image)\n",
        "            batch_steering_angles.append(steering_angle)\n",
        "            \n",
        "        yield( np.asarray(batch_images), np.asarray(batch_steering_angles))"
      ],
      "metadata": {
        "id": "6ctFRd-ohpRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ncol = 2\n",
        "nrow = 2\n",
        "\n",
        "X_train_batch, y_train_batch = next(image_data_generator(X_train, y_train, nrow, True))\n",
        "X_valid_batch, y_valid_batch = next(image_data_generator(X_valid, y_valid, nrow, False))\n",
        "\n",
        "fig, axes = plt.subplots(nrow, ncol, figsize=(15, 6))\n",
        "fig.tight_layout()\n",
        "\n",
        "for i in range(nrow):\n",
        "    axes[i][0].imshow(X_train_batch[i])\n",
        "    axes[i][0].set_title(\"training, angle=%s\" % y_train_batch[i])\n",
        "    axes[i][1].imshow(X_valid_batch[i])\n",
        "    axes[i][1].set_title(\"validation, angle=%s\" % y_valid_batch[i])"
      ],
      "metadata": {
        "id": "Esz0P4-uhr3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean up log folder for tensorboard\n",
        "log_dir_root = f'{model_output_dir}/logs/'\n",
        "#!rm -rf $log_dir_root\n",
        "\n",
        "# this block prevents the training from starting if we Run All\n",
        "#DO_NOT_RUN_ALL\n",
        "\n",
        "# saves the model weights after each epoch if the validation loss decreased\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=os.path.join(model_output_dir,'roadCam_navigation_check.h5'), verbose=1, save_best_only=True)\n",
        "\n",
        "history = model.fit_generator(image_data_generator( X_train, y_train, batch_size=100, is_training=True),\n",
        "                              steps_per_epoch=300,\n",
        "                              epochs=10,\n",
        "                              validation_data = image_data_generator( X_valid, y_valid, batch_size=100, is_training=False),\n",
        "                              validation_steps=200,\n",
        "                              verbose=1,\n",
        "                              shuffle=1,\n",
        "                              callbacks=[checkpoint_callback])\n",
        "# always save model output as soon as model finishes training\n",
        "model.save(os.path.join(model_output_dir,'roadCam_navigation_final.h5'))\n",
        "\n",
        "date_str = datetime.datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
        "history_path = os.path.join(model_output_dir,'history.pickle')\n",
        "with open(history_path, 'wb') as f:\n",
        "    pickle.dump(history.history, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "wU2YSkcHhtt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.history.keys())\n",
        "print(history.history)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zMdbAgUfORwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "objects = []\n",
        "with (open(\"history.pickle\", \"rb\")) as openfile:\n",
        "    while True:\n",
        "        try:\n",
        "            objects.append(pickle.load(openfile))\n",
        "        except EOFError:\n",
        "            break"
      ],
      "metadata": {
        "id": "Yyu0-3o9RV7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(objects[0].keys())\n",
        "print(objects[0])\n",
        "plt.plot(objects[0]['loss'])\n",
        "plt.plot(objects[0]['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aJnendFOQybF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "print(f'{model_output_dir}')\n",
        "testing = '/content/gdrive/My Drive/Colab Notebooks/roadCamCar/output/roadCam_navigation_check.h5'\n",
        "\n",
        "def summarize_prediction(Y_true, Y_pred):\n",
        "    mse = mean_squared_error(Y_true, Y_pred)\n",
        "    r_squared = r2_score(Y_true, Y_pred)\n",
        "    print(f'mse       = {mse:.2}')\n",
        "    print(f'r_squared = {r_squared:.2%}')\n",
        "    \n",
        "def predict_and_summarize(X, Y):\n",
        "    model = load_model(testing, compile = False)\n",
        "    Y_pred = model.predict(X)\n",
        "    summarize_prediction(Y, Y_pred)\n",
        "    return Y_pred\n",
        "  \n",
        "n_tests = 100\n",
        "X_test, y_test = next(image_data_generator(X_valid, y_valid, n_tests, False))\n",
        "y_pred = predict_and_summarize(X_test, y_test)"
      ],
      "metadata": {
        "id": "OWd8kXQI2pTz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}